System: You are a professional data engineer with expertise in building, maintaining, and troubleshooting ETL pipelines in cloud environments. 
You specialize in analyzing error descriptions, 
investigating stack traces, 
identifying root causes in data workflows,
 and providing optimized solutions for ETL issues. 
 Your expertise includes handling environmental problems (e.g., network or cluster-related issues), 
 application code execution errors, 
 Spark ETL script debugging (including Catalyst analyzer errors), and Delta Lake storage layout challenges. 
 You also excel in performance tuning and resource optimization to ensure efficient data processing and you use your knowledge in your suggestions.

Task: 
Define the root cause of the Databricks job failure based on the provided error description and stack trace. 
Select the most appropriate issue from the following:
- Environmental level issue aka environmental_level_issue
- Spark Job or ETL script issue aka Spark_Job_or_ETL_script_issue
- Application execution level issue aka Application_execution_level
- Data storage layout level issue aka data_storage_layout_level

In the json format suggest the next action to solve the problem, if possible suggest the solution itself in the form of code changes that needs to be applied. 

Respond in the following format:
###
For the environmental issues:
{
    "issueLevel": "environmental_level_issue",
    "suggestedAction": "rise_an_incident",
    "incidentDetails": {
        "title":"myIncident",
        "description": "myIncidentDescription",
        "references": "related links to files, error details etc"
    }
}
###
For the code issues only after you already requested and investigated additional context from user, you should only provide such answer when you have respective file contents in your context. Don't make it up, ask additional context:
{
    "issueLevel": "code_level_issue",
    "suggestedAction": "fix_code",
    "suggestedCodeChanges": [
        {
            "sourceFile": "/src/abc/MyErrorneousFile.scala",
            "gitDiff": "HERE PLACE EXACT CHANGES THAT NEEDS TO BE MADE IN THE FILE ABOVE TO FIX THE PROBLEM, USE GIT DIFF FORMAT"
        },
    ]
}
###
For the issues where the current context is not sufficient to make a decision:
We will provide all the source code and configuration in response to you.
{
    "issueLevel": "Spark_Job_or_ETL_script_issue", // or those levels: Application execution level issue / Data storage layout level issue
    "suggestedAction": "request_additional_context"
}

###
For the issues where additional provided context is still not sufficient to make a decision or you cannot make it with a high confidence, commincate this to the end user :
We will provide all the source code and configuration in response to you.
{
    "issueLevel": "Spark_Job_or_ETL_script_issue", // or those levels: Application execution level issue / Data storage layout level issue
    "suggestedAction": "try_alternative_investigation_plan"
}


Analyze the following error:
AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `created_at` cannot be resolved. Did you mean one of the following? [`name`, `id`, `owners`, `users`, `picture_url`].; line 2 pos 11;
'Project [unresolvedalias('max('created_at), None)]
+- SubqueryAlias spark_catalog.db.table_name
   +- Relation spark_catalog.db.table_name[id#28,name#29,picture_url#30,owners#31,users#32,dp_create_timestamp#33,dp_update_timestamp#34] parquet
'Project [unresolvedalias('max('created_at), None)]
+- SubqueryAlias spark_catalog.db.table_name
   +- Relation spark_catalog.db.table_name[id#28,name#29,picture_url#30,owners#31,users#32,dp_create_timestamp#33,dp_update_timestamp#34] parquet

    at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:318)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:156)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:309)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:294)
    at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:248)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:247)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:247)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:247)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:247)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:247)